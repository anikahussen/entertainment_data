<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <title>Assignment 1: Research </title>
  <link rel="stylesheet" type="text/css" href="../custom.css">
    <style>
      #title {
        text-align: center;

      }
      #cogfloat {
        margin-left: 80%;
        margin-top: 15%;
        position: fixed;
        float: left;

      }

  

    </style>

  </head>
  <img id="cogfloat" src="cog.gif" alt="cogs">

<body id="assignmentbody">

<a id="home" href=../index.html><img src="home.png" width="64px" height="64px"></a>
<h1 id="title">R e c e n t &nbsp;&nbsp; D a t a b a s e &nbsp;&nbsp;E v e n t s</h1>

<div id="researchpage">
<table>
<tr >
  <td colspan="2" class="title">
<h3 >"New Pressure on Google and YouTube Over Children’s Data"</h3>
<h4>New York Times</h4>
<h5>Sapna Maheshwari<br>Sept. 20, 2018</h5>
  </td>

</tr>
<tr>
  <td>
<p>
It is obvious that Google has access to a mass of our online data. Particularly through YouTube, since it has become a large entertainment and search platform. From a business perspective, catering advertisement to the individual user is extremely beneficial. The algorithm can easily adapt to the changing consumer interests since the data is available through the frequent reliance on the internet. However, there exists a legal conflict between tracking the online behavior of children 13 and younger for target advertisement and the Children Online Privacy Protection Act. The YouTube guidelines do specify that the product is for people older than 13; however, government officials like Democrat David Cicilline of Rhode Island question why there is not a secure gateway to prevent children from YouTube access. YouTube explicitly shows that they prohibit “’interest-based advertisements’ and ads with ‘tracking pixels’” on the YouTube Kids app, but that does not involve the data-driven advertising algorithm.

 </p>
    </td>

    <td>
<img src="youtube_screen.jpg " alt="Youtube Screen">
      </td>

</tr>

<tr>
  <td colspan="2">
  <img id="letterphoto" class="photo" src="children_letter.jpg" alt="Children Letter" >
</td>
  </tr>


<tr>

    <td colspan="2">
<p>
Comments, views, and activity, not only to train the YouTube AI to recommend the best channels for the user but also to tailor advertisements. The algorithm factors in watch time, impression on clicks from the recommendations and viewer engagement.  As the algorithm runs through the YouTube database, it picks up on individual's and group's patterns. Government officials advocating for children’s privacy in the digital world believe that there should be more marketing regulation flags that stop both the marketers and algorithm from venturing into children data. Even legal actions regulate the data the YouTube factors into its algorithm, there is far more avenues for data collection through Google’s enterprises. To combat the flaws in the algorithm, officials and users encourage the addition and improvement of flags. Restrictions like age limit and parental permission can block data from the AI learning process. YouTube would still have control over targeted advertisement based on the general trends among the YouTube Kids community, rather than each search.



</p>

<a class="link" href="https://www.nytimes.com/2018/09/20/business/media/google-youtube-children-data.html "><p id="sl">See Link</p></a>


      </td>

</tr>


</table>




<table >
<tr>
  <td colspan="2" class="title">
<h3>"Amazon scraps secret AI recruiting tool that showed bias against women"</h3>
<h4>Reuters</h4>
<h5>Jeffrey Dastin <br>October 9, 2018</h5>
    </td>

</tr>

<tr>
  <td>
<p>
The tech industry has notoriously been male-dominant, but currently, there are efforts to move out of the discriminating patterns. However, as big data and machine learning become a primary way of quickening up task performance, large corporations like Amazon and Reuters have dedicated the first recruiting processes to machines.  At first, the purely data-based process of weeding out applicants seemed fairer and more objective compared to human evaluation based on natural biases. Amazon and Reuters provided their recruiting engines with a 1 to 5 stars system, where the machines would rate the applicants. Soon, the companies, particularly Amazon, noticed that the data used to teach the recruiting engine did not provoke “gender neutral” decision making. The training data consisted of male-dominated resumes. Thus, the recruiting engines discriminated against keywords like “women”. For example, the algorithmic engine downgraded a resume that mentioned graduating from “all-women colleges”. For factors like social stereotyping among others, there is a language nuance between genders including simply using the words like “executed” and “captured”, are conventionally “masculine”. Patterns show that female applicants do not incorporate “masculine” terms, which the recruiting machine sees as one sign of an unsuccessful applicant.


 </p>
    </td>

    <td>
<img class="photo" src="amazon_bias.jpg" alt="amazon robot">
      </td>

</tr>

<tr>
  <td>
    <img class="photo" src="recruiting.jpg" alt="amazon recruiting conference">

    </td>

    <td>
<p>

Unlike the entertainment algorithms like used for YouTube, the abundant amount of computer models (ie. 500 models in specific Amazon and Reuter locations), do not help refine the discriminatory results. The data from the male-dominated history of the tech industry will only continue the gender-bias tradition. New representative data is necessary to make fair conclusions. Aside from gender, the tech industry should not heavily rely on historical patterns as the industry, itself, personifies advancement and adaptations. Applicants from ten years ago will inevitably look different from the applicant ten years from now. Amazon and Reuters now do not heavily rely on the recruiting engine, but instead, use the “’watered-down’” method to carry out menial tasks like removing duplicates from the databases. For e-commerce companies like Amazon, quick production becomes a priority; thus, machine learning becomes an attractive tool fitting the Amazon efficiency. Yet, bias in data can provoke the danger of continuing on with the historical prejudices and mistakes.


</p>

<a class="link" href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G "><p id="sl">See Link</p></a>
      </td>
      </td>

</tr>

<tr>
  <td colspan="2" class="title">
<h3>"Science & Human Rights Coalition Meeting: Big Data & Human Rights"</h3>
<h4>American Association For The Advancement of Science</h4>
<h5>January 15-16, 2018 </h5>

    </td>

</tr>
<tr>
  <td>
<p>
There is an understanding gap between the government, technology and data science that, in itself, causes problems like lack of terminology and cybersecurity without the use of specific datasets—venturing into the privacy invasion dilemma. Yet, investigating datasets can allow organizations to combat human rights violations around the world. The Human Rights Watch were able to learn about the chlorine barrel bombs in Syria through YouTube videos and satellite imagery. On the other side of the discriminatory data bias, Emmanuel Letouzé, founder of Data Pop Alliance, points out that an essential risk in organizing data is “re-identification and de-anonymization of information.” Without the re-analyzing of recent global data, officials would not be able to adapt their actions to combat the issues occurring. The growing prevalence of big data provided insight into the youth homelessness conditions in Illinois through the Chicago Coalition data. Access to data about new social phenomena could excel the process of constructing mechanisms to address issues, rather than relying on assumptions. On the contrary to fighting human rights violation, Jeramie Scott, National Security Counsel at the Electronic Privacy Information Center, brings back the privacy discussion surrounding big data.


 </p>
    </td>

    <td>
      <img class="photo" src="chicago_coalition.jpg" alt="Chicago Coalition">

      </td>

</tr>

<tr>
  <td>
    <img class="data_map_photo" src="human_rights_data.jpg" alt="Human Rights Data">

    </td>

    <td>
<p>

  With large connective technological systems like toll highways and social media, there is more data access than ever. On the other hand, information from the internet and media sources tend to have discrepancies in human rights violations, among other important issues. Mass data collection is helpful in preventing misinterpretation or assuming false information. Unlike the earlier discussion surrounding private corporation use of big data, political and human rights use of data seems to have more justification. However, some of the data is still extracted from corporate run platforms like YouTube, where ownership and privacy become blurred. The rapid mass of data and tech advancement plays into the misunderstanding with the government. Similar to the way big data and machine learning relies on historical patterns, preceding cases advise governmental decisions. However, in the digital age, there is a massive unprecedented area. There are not, yet, clear legislation on how to deal with privacy and the situational helpful access to data. Not only do officials have to deal with the evolving data world and technology, but there must also be discussions between the public, private corporations, and government officials to meet at a compromise.




</p>

<a class="link" href="https://www.aaas.org/events/science-human-rights-coalition-meeting-big-data-human-rights"><p id="sl" >See Link</p></a>
      </td>

</tr>


</table>
















</div>
</body>













</html>
